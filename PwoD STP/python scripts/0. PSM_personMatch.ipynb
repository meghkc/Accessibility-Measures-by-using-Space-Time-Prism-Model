{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hptripid</th>\n",
       "      <th>tripID</th>\n",
       "      <th>hhmemberid</th>\n",
       "      <th>o_purp_t</th>\n",
       "      <th>depart_hhm</th>\n",
       "      <th>mode_t</th>\n",
       "      <th>hhsize</th>\n",
       "      <th>workers</th>\n",
       "      <th>hh_income</th>\n",
       "      <th>num_veh</th>\n",
       "      <th>...</th>\n",
       "      <th>d_CoTAZID_v30</th>\n",
       "      <th>ACRES</th>\n",
       "      <th>DEVACRES</th>\n",
       "      <th>DEVPBLEPCT</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>CO_NAME</th>\n",
       "      <th>CITY_NAME</th>\n",
       "      <th>group</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22717</th>\n",
       "      <td>W59245FU.01.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W59245FU.01</td>\n",
       "      <td>home</td>\n",
       "      <td>600</td>\n",
       "      <td>auto</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>110160</td>\n",
       "      <td>80.420980</td>\n",
       "      <td>80.420980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>418233.184712</td>\n",
       "      <td>4.544694e+06</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>Layton</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>M19607WR.01.01</td>\n",
       "      <td>1</td>\n",
       "      <td>M19607WR.01</td>\n",
       "      <td>home</td>\n",
       "      <td>1050</td>\n",
       "      <td>auto</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>490359</td>\n",
       "      <td>206.599606</td>\n",
       "      <td>206.599606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>429023.318978</td>\n",
       "      <td>4.474159e+06</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>Lehi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25132</th>\n",
       "      <td>W67581HT.01.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W67581HT.01</td>\n",
       "      <td>home</td>\n",
       "      <td>1800</td>\n",
       "      <td>auto</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>570279</td>\n",
       "      <td>165.725154</td>\n",
       "      <td>165.725154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>420098.405715</td>\n",
       "      <td>4.561620e+06</td>\n",
       "      <td>WEBER</td>\n",
       "      <td>Ogden</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26926</th>\n",
       "      <td>W74129NS.01.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W74129NS.01</td>\n",
       "      <td>home</td>\n",
       "      <td>810</td>\n",
       "      <td>auto</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>350124</td>\n",
       "      <td>97.783061</td>\n",
       "      <td>97.783061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>429192.027786</td>\n",
       "      <td>4.507498e+06</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16610</th>\n",
       "      <td>W38073MU.01.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W38073MU.01</td>\n",
       "      <td>home</td>\n",
       "      <td>640</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>350729</td>\n",
       "      <td>85.055080</td>\n",
       "      <td>85.055080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>425655.822985</td>\n",
       "      <td>4.504744e+06</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>South Salt Lake</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27312</th>\n",
       "      <td>W75664TR.02.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W75664TR.02</td>\n",
       "      <td>home</td>\n",
       "      <td>755</td>\n",
       "      <td>auto</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>350501</td>\n",
       "      <td>111.135645</td>\n",
       "      <td>111.135645</td>\n",
       "      <td>1.0</td>\n",
       "      <td>429165.308640</td>\n",
       "      <td>4.506920e+06</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23430</th>\n",
       "      <td>W61733SZ.01.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W61733SZ.01</td>\n",
       "      <td>home</td>\n",
       "      <td>745</td>\n",
       "      <td>auto</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>350272</td>\n",
       "      <td>43.265705</td>\n",
       "      <td>43.265705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>426845.851127</td>\n",
       "      <td>4.513154e+06</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23729</th>\n",
       "      <td>W62747DT.02.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W62747DT.02</td>\n",
       "      <td>home</td>\n",
       "      <td>700</td>\n",
       "      <td>auto</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>570396</td>\n",
       "      <td>409.555912</td>\n",
       "      <td>409.555912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>419789.516914</td>\n",
       "      <td>4.540563e+06</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>Kaysville</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>M25782NC.05.01</td>\n",
       "      <td>1</td>\n",
       "      <td>M25782NC.05</td>\n",
       "      <td>home</td>\n",
       "      <td>945</td>\n",
       "      <td>auto</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>490725</td>\n",
       "      <td>31.125857</td>\n",
       "      <td>31.125857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>445147.585421</td>\n",
       "      <td>4.454523e+06</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>Provo</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21810</th>\n",
       "      <td>W56297EH.02.01</td>\n",
       "      <td>1</td>\n",
       "      <td>W56297EH.02</td>\n",
       "      <td>home</td>\n",
       "      <td>925</td>\n",
       "      <td>auto</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>570370</td>\n",
       "      <td>200.328379</td>\n",
       "      <td>200.328379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>411900.009383</td>\n",
       "      <td>4.556732e+06</td>\n",
       "      <td>WEBER</td>\n",
       "      <td>Roy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>730 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             hptripid  tripID   hhmemberid o_purp_t  depart_hhm mode_t  \\\n",
       "22717  W59245FU.01.01       1  W59245FU.01     home         600   auto   \n",
       "3033   M19607WR.01.01       1  M19607WR.01     home        1050   auto   \n",
       "25132  W67581HT.01.01       1  W67581HT.01     home        1800   auto   \n",
       "26926  W74129NS.01.01       1  W74129NS.01     home         810   auto   \n",
       "16610  W38073MU.01.01       1  W38073MU.01     home         640   auto   \n",
       "...               ...     ...          ...      ...         ...    ...   \n",
       "27312  W75664TR.02.01       1  W75664TR.02     home         755   auto   \n",
       "23430  W61733SZ.01.01       1  W61733SZ.01     home         745   auto   \n",
       "23729  W62747DT.02.01       1  W62747DT.02     home         700   auto   \n",
       "5155   M25782NC.05.01       1  M25782NC.05     home         945   auto   \n",
       "21810  W56297EH.02.01       1  W56297EH.02     home         925   auto   \n",
       "\n",
       "       hhsize  workers  hh_income  num_veh  ...  d_CoTAZID_v30       ACRES  \\\n",
       "22717       7        1         -1        2  ...         110160   80.420980   \n",
       "3033        3        3         -1        3  ...         490359  206.599606   \n",
       "25132       2        2          3        2  ...         570279  165.725154   \n",
       "26926       2        2          4        2  ...         350124   97.783061   \n",
       "16610       1        1          5        2  ...         350729   85.055080   \n",
       "...       ...      ...        ...      ...  ...            ...         ...   \n",
       "27312       2        1          4        1  ...         350501  111.135645   \n",
       "23430       2        2          5        2  ...         350272   43.265705   \n",
       "23729       3        2         -1        4  ...         570396  409.555912   \n",
       "5155        6        1          1        5  ...         490725   31.125857   \n",
       "21810       5        1         -1        5  ...         570370  200.328379   \n",
       "\n",
       "         DEVACRES  DEVPBLEPCT              X             Y    CO_NAME  \\\n",
       "22717   80.420980         1.0  418233.184712  4.544694e+06      DAVIS   \n",
       "3033   206.599606         1.0  429023.318978  4.474159e+06       UTAH   \n",
       "25132  165.725154         1.0  420098.405715  4.561620e+06      WEBER   \n",
       "26926   97.783061         1.0  429192.027786  4.507498e+06  SALT LAKE   \n",
       "16610   85.055080         1.0  425655.822985  4.504744e+06  SALT LAKE   \n",
       "...           ...         ...            ...           ...        ...   \n",
       "27312  111.135645         1.0  429165.308640  4.506920e+06  SALT LAKE   \n",
       "23430   43.265705         1.0  426845.851127  4.513154e+06  SALT LAKE   \n",
       "23729  409.555912         1.0  419789.516914  4.540563e+06      DAVIS   \n",
       "5155    31.125857         1.0  445147.585421  4.454523e+06       UTAH   \n",
       "21810  200.328379         1.0  411900.009383  4.556732e+06      WEBER   \n",
       "\n",
       "             CITY_NAME  group  propensity_score  \n",
       "22717           Layton      1          0.994008  \n",
       "3033              Lehi      1          0.995202  \n",
       "25132            Ogden      1          0.995978  \n",
       "26926   Salt Lake City      1          0.995822  \n",
       "16610  South Salt Lake      1          0.990316  \n",
       "...                ...    ...               ...  \n",
       "27312   Salt Lake City      1          0.990423  \n",
       "23430   Salt Lake City      1          0.995822  \n",
       "23729        Kaysville      1          0.993281  \n",
       "5155             Provo      1          0.995234  \n",
       "21810              Roy      1          0.992863  \n",
       "\n",
       "[730 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "# Function to compute propensity scores\n",
    "def compute_propensity_scores(df, relevant_columns):\n",
    "    X = df[relevant_columns]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, df['group'])\n",
    "    scores = model.predict_proba(X)[:, 1]\n",
    "    return scores\n",
    "\n",
    "def propensity_score_matching(control, treatment, caliper):\n",
    "    matched_treatment = pd.DataFrame()\n",
    "    matched_treatment_indices = set()\n",
    "\n",
    "    for index, control_row in control.iterrows():\n",
    "        control_score = control_row['propensity_score']\n",
    "        potential_matches = treatment[(treatment['propensity_score'] >= control_score - caliper) &\n",
    "                                      (treatment['propensity_score'] <= control_score + caliper) &\n",
    "                                      (~treatment.index.isin(matched_treatment_indices))\n",
    "                                      ]\n",
    "        if not potential_matches.empty:\n",
    "            # Select up to 5 matches without replacement\n",
    "            selected_matches = potential_matches.sample(n=min(5, len(potential_matches)), replace=False)\n",
    "            matched_treatment = pd.concat([matched_treatment, selected_matches])\n",
    "            matched_treatment_indices.update(selected_matches.index)\n",
    "\n",
    "            # Add matched treatment indices to the set to avoid re-matching\n",
    "            matched_treatment_indices.update(selected_matches.index)\n",
    "\n",
    "    return matched_treatment\n",
    "\n",
    "# Load the datasets\n",
    "control_group_df = pd.read_csv(fr\"E:\\Propensity score matching analysis\\TAZ_origin.csv\")\n",
    "treatment_group_df = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD.csv\")\n",
    "\n",
    "# # Remove null or NA values from CO_NAME column\n",
    "# control_group_df.dropna(subset=['CO_NAME'], inplace=True)\n",
    "# treatment_group_df.dropna(subset=['CO_NAME'], inplace=True)\n",
    "\n",
    "# Unique values for emp_text column in control_group_df\n",
    "control_emp_values = control_group_df['emp_text'].unique()\n",
    "# Filtering treatment_group_df to have only those values in control_emp_values\n",
    "treatment_group_df = treatment_group_df[treatment_group_df['emp_text'].isin(control_emp_values)]\n",
    "\n",
    "# # Unique values for CO_NAME column in control_group_df\n",
    "# control_CO_values = control_group_df['CO_NAME'].unique()\n",
    "# # Filtering treatment_group_df to have only those values in control_emp_values\n",
    "# treatment_group_df = treatment_group_df[treatment_group_df['CO_NAME'].isin(control_CO_values)]\n",
    "\n",
    "# Keep only unique values in the 'hhmemberid' column\n",
    "control_group_df = control_group_df.drop_duplicates(subset=['hhmemberid'])\n",
    "treatment_group_df = treatment_group_df.drop_duplicates(subset=['hhmemberid'])\n",
    "\n",
    "# define categorical variables\n",
    "cat=['age', 'gender', 'education', 'licensed', 'emp_text']\n",
    "# define continuous variables\n",
    "con=['num_veh', 'workers', 'hhsize']\n",
    "\n",
    "# Define relevant columns for the analysis\n",
    "relevant_columns = cat+con\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in cat:\n",
    "    if control_group_df[col].dtype == object:\n",
    "        control_group_df[col] = le.fit_transform(control_group_df[col])\n",
    "        treatment_group_df[col] = le.transform(treatment_group_df[col])\n",
    "\n",
    "# Assign group labels\n",
    "control_group_df = control_group_df.assign(group=0)\n",
    "treatment_group_df = treatment_group_df.assign(group=1)\n",
    "\n",
    "# Combine subsets for propensity score computation\n",
    "combined_subset = pd.concat([control_group_df, treatment_group_df])\n",
    "combined_subset['propensity_score'] = compute_propensity_scores(combined_subset, relevant_columns)\n",
    "\n",
    "# Split the combined subset back into control and treatment with propensity scores\n",
    "control_subset = combined_subset[combined_subset['group'] == 0]\n",
    "treatment_subset = combined_subset[combined_subset['group'] == 1]\n",
    "\n",
    "# Define caliper as 0.25 standard deviations of the propensity score\n",
    "caliper = 0.25 * combined_subset['propensity_score'].std()\n",
    "\n",
    "matched_subset = propensity_score_matching(control_subset, treatment_subset, caliper)\n",
    "\n",
    "display(matched_subset)\n",
    "#export it as csv\n",
    "matched_subset.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_PwoD_PSM_hhmember.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of Propensity Scores in the Matched Dataset: 0.11174607603567492\n"
     ]
    }
   ],
   "source": [
    "# Calculate the standard deviation of the propensity scores in the combined matched dataset\n",
    "propensity_score_std = matched_subset['propensity_score'].std()\n",
    "print(\"Standard Deviation of Propensity Scores in the Matched Dataset:\", propensity_score_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trips after matching the person data is: 1923\n"
     ]
    }
   ],
   "source": [
    "### Keep all matching hhmemberid from the initial treatment dataset\n",
    "# Load the initial_treatment dataset\n",
    "initial_treatment_orig = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD.csv\")\n",
    "initial_treatment_dest = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD.csv\")\n",
    "# Merge the two dataframes on hhmemberid\n",
    "matching_rows_orig = pd.merge(initial_treatment_orig, matched_subset[['hhmemberid']], on='hhmemberid', how='inner')\n",
    "matching_rows_dest = pd.merge(initial_treatment_dest, matched_subset[['hhmemberid']], on='hhmemberid', how='inner')\n",
    "print('number of trips after matching the person data is:',len(matching_rows_orig))\n",
    "# Save the combined matched dataset to a CSV file\n",
    "matching_rows_orig.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM1.csv\", index=False)\n",
    "matching_rows_dest.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique hptripid in origin: 1923\n",
      "Number of unique hptripid in destination: 1923\n"
     ]
    }
   ],
   "source": [
    "#### Keeping only matching hptripids in origin and destination dataset\n",
    "# Load the datasets\n",
    "TAZ_origin_df = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM1.csv\")\n",
    "TAZ_destination_df = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM1.csv\")\n",
    "\n",
    "# Finding unique hptripid in each file\n",
    "unique_origin = set(TAZ_origin_df['hptripid'].unique())\n",
    "print(\"Number of unique hptripid in origin:\", len(unique_origin))\n",
    "unique_destination = set(TAZ_destination_df['hptripid'].unique())\n",
    "print(\"Number of unique hptripid in destination:\", len(unique_destination))\n",
    "\n",
    "# Common hptripid in both origin and destination files\n",
    "common_hptripid = unique_origin.intersection(unique_destination)\n",
    "num_common_hptripid = len(common_hptripid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common hptripid in both files: 1923\n",
      "Number of unique hhmemberid in origin: 730\n",
      "Number of unique hhmemberid in destination: 730\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keeping only common hptripid in both files\n",
    "TAZ_origin_common = TAZ_origin_df[TAZ_origin_df['hptripid'].isin(common_hptripid)]\n",
    "TAZ_destination_common = TAZ_destination_df[TAZ_destination_df['hptripid'].isin(common_hptripid)]\n",
    "# Saving the matching hptripd origin and destination dataset data as a CSV file\n",
    "TAZ_origin_common.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM2.csv\", index=False)\n",
    "TAZ_destination_common.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM2.csv\", index=False)\n",
    "# Print the results\n",
    "print(\"Number of common hptripid in both files:\", num_common_hptripid)\n",
    "\n",
    "# Finding unique hhmemberid in each Final dataset after PSM mathching and matching common hptripids\n",
    "# Load the datasets\n",
    "TAZ_origin_df1 = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM2.csv\")\n",
    "TAZ_destination_df1 = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM2.csv\")\n",
    "unique_origin1 = set(TAZ_origin_df1['hhmemberid'].unique())\n",
    "print(\"Number of unique hhmemberid in origin:\", len(unique_origin1))\n",
    "unique_destination2 = set(TAZ_destination_df1['hhmemberid'].unique())\n",
    "print(\"Number of unique hhmemberid in destination:\", len(unique_destination2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
