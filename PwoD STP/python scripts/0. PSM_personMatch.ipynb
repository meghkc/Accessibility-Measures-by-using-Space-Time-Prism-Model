{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "# Function to compute propensity scores\n",
    "def compute_propensity_scores(df, relevant_columns):\n",
    "    X = df[relevant_columns]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, df['group'])\n",
    "    scores = model.predict_proba(X)[:, 1]\n",
    "    return scores\n",
    "\n",
    "def propensity_score_matching(control, treatment, caliper):\n",
    "    matched_treatment = pd.DataFrame()\n",
    "    matched_treatment_indices = set()\n",
    "\n",
    "    for index, control_row in control.iterrows():\n",
    "        control_score = control_row['propensity_score']\n",
    "        potential_matches = treatment[(treatment['propensity_score'] >= control_score - caliper) &\n",
    "                                      (treatment['propensity_score'] <= control_score + caliper) &\n",
    "                                      (~treatment.index.isin(matched_treatment_indices))\n",
    "                                      ]\n",
    "        if not potential_matches.empty:\n",
    "            # Select up to 5 matches without replacement\n",
    "            selected_matches = potential_matches.sample(n=min(5, len(potential_matches)), replace=False)\n",
    "            matched_treatment = pd.concat([matched_treatment, selected_matches])\n",
    "            matched_treatment_indices.update(selected_matches.index)\n",
    "\n",
    "            # Add matched treatment indices to the set to avoid re-matching\n",
    "            matched_treatment_indices.update(selected_matches.index)\n",
    "\n",
    "    return matched_treatment\n",
    "\n",
    "# Load the datasets\n",
    "control_group_df = pd.read_csv(fr\"E:\\Propensity score matching analysis\\TAZ_destination.csv\")\n",
    "treatment_group_df = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD.csv\")\n",
    "\n",
    "# # Remove null or NA values from CO_NAME column\n",
    "# control_group_df.dropna(subset=['CO_NAME'], inplace=True)\n",
    "# treatment_group_df.dropna(subset=['CO_NAME'], inplace=True)\n",
    "\n",
    "# Unique values for emp_text column in control_group_df\n",
    "control_emp_values = control_group_df['emp_text'].unique()\n",
    "# Filtering treatment_group_df to have only those values in control_emp_values\n",
    "treatment_group_df = treatment_group_df[treatment_group_df['emp_text'].isin(control_emp_values)]\n",
    "\n",
    "# # Unique values for CO_NAME column in control_group_df\n",
    "# control_CO_values = control_group_df['CO_NAME'].unique()\n",
    "# # Filtering treatment_group_df to have only those values in control_emp_values\n",
    "# treatment_group_df = treatment_group_df[treatment_group_df['CO_NAME'].isin(control_CO_values)]\n",
    "\n",
    "# Keep only unique values in the 'hhmemberid' column\n",
    "control_group_df = control_group_df.drop_duplicates(subset=['hhmemberid'])\n",
    "treatment_group_df = treatment_group_df.drop_duplicates(subset=['hhmemberid'])\n",
    "\n",
    "# define categorical variables\n",
    "cat=['age', 'gender', 'education', 'licensed', 'emp_text']\n",
    "# define continuous variables\n",
    "con=['num_veh', 'workers', 'hhsize']\n",
    "\n",
    "# Define relevant columns for the analysis\n",
    "relevant_columns = cat+con\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in cat:\n",
    "    if control_group_df[col].dtype == object:\n",
    "        control_group_df[col] = le.fit_transform(control_group_df[col])\n",
    "        treatment_group_df[col] = le.transform(treatment_group_df[col])\n",
    "\n",
    "# Assign group labels\n",
    "control_group_df = control_group_df.assign(group=0)\n",
    "treatment_group_df = treatment_group_df.assign(group=1)\n",
    "\n",
    "# Combine subsets for propensity score computation\n",
    "combined_subset = pd.concat([control_group_df, treatment_group_df])\n",
    "combined_subset['propensity_score'] = compute_propensity_scores(combined_subset, relevant_columns)\n",
    "\n",
    "# Split the combined subset back into control and treatment with propensity scores\n",
    "control_subset = combined_subset[combined_subset['group'] == 0]\n",
    "treatment_subset = combined_subset[combined_subset['group'] == 1]\n",
    "\n",
    "# Define caliper as 0.25 standard deviations of the propensity score\n",
    "caliper = 0.25 * combined_subset['propensity_score'].std()\n",
    "\n",
    "matched_subset = propensity_score_matching(control_subset, treatment_subset, caliper)\n",
    "\n",
    "display(matched_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of the propensity scores in the combined matched dataset\n",
    "propensity_score_std = matched_subset['propensity_score'].std()\n",
    "print(\"Standard Deviation of Propensity Scores in the Matched Dataset:\", propensity_score_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep all matching hhmemberid from the initial treatment dataset\n",
    "# Load the initial_treatment dataset\n",
    "initial_treatment = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD.csv\")\n",
    "# Merge the two dataframes on hhmemberid\n",
    "matching_rows = pd.merge(initial_treatment, matched_subset[['hhmemberid']], on='hhmemberid', how='inner')\n",
    "print('number of trips after matching the person data is:',len(matching_rows))\n",
    "# Save the combined matched dataset to a CSV file\n",
    "matching_rows.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Keeping only matching hptripids in origin and destination dataset\n",
    "# Load the datasets\n",
    "TAZ_origin_df = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM1.csv\")\n",
    "TAZ_destination_df = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM1.csv\")\n",
    "\n",
    "# Finding unique hptripid in each file\n",
    "unique_origin = set(TAZ_origin_df['hptripid'].unique())\n",
    "print(\"Number of unique hptripid in origin:\", len(unique_origin))\n",
    "unique_destination = set(TAZ_destination_df['hptripid'].unique())\n",
    "print(\"Number of unique hptripid in destination:\", len(unique_destination))\n",
    "\n",
    "# Common hptripid in both origin and destination files\n",
    "common_hptripid = unique_origin.intersection(unique_destination)\n",
    "num_common_hptripid = len(common_hptripid)\n",
    "\n",
    "# Keeping only common hptripid in both files\n",
    "TAZ_origin_common = TAZ_origin_df[TAZ_origin_df['hptripid'].isin(common_hptripid)]\n",
    "TAZ_destination_common = TAZ_destination_df[TAZ_destination_df['hptripid'].isin(common_hptripid)]\n",
    "# Saving the matching hptripd origin and destination dataset data as a CSV file\n",
    "TAZ_origin_common.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM2.csv\", index=False)\n",
    "TAZ_destination_common.to_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM2.csv\", index=False)\n",
    "# Print the results\n",
    "print(\"Number of common hptripid in both files:\", num_common_hptripid)\n",
    "\n",
    "# Finding unique hhmemberid in each Final dataset after PSM mathching and matching common hptripids\n",
    "# Load the datasets\n",
    "TAZ_origin_df1 = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_origin_PwoD_PSM2.csv\")\n",
    "TAZ_destination_df1 = pd.read_csv(fr\"E:\\STP_PWoD\\Data\\TAZ_destination_PwoD_PSM2.csv\")\n",
    "unique_origin1 = set(TAZ_origin_df1['hhmemberid'].unique())\n",
    "print(\"Number of unique hhmemberid in origin:\", len(unique_origin1))\n",
    "unique_destination2 = set(TAZ_destination_df1['hhmemberid'].unique())\n",
    "print(\"Number of unique hhmemberid in destination:\", len(unique_destination2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
